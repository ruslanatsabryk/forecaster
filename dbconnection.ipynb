{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "db81cc12-749a-420b-b374-6de5302430fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oracledb\n",
    "import cx_Oracle\n",
    "from datetime import datetime, timedelta, time, date\n",
    "import numpy as np\n",
    "from scipy.interpolate import splrep, splev\n",
    "import csv\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import mean_squared_log_error \n",
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from tpot.builtins import StackingEstimator\n",
    "from tpot.export_utils import set_param_recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18df540-2d08-4d23-a7a3-93cd574ef156",
   "metadata": {},
   "source": [
    "#### Функция чтения данных из БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e588084-62e8-4218-bcbd-bb6cb3895a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_temperature_from_db(start_date, finish_date):\n",
    "\n",
    "    try:\n",
    "        cx_Oracle.init_oracle_client(lib_dir=r\"F:\\ruslan\\dev\\oracle\\instantclient_19_9\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    conn_nwp = oracledb.connect(user=\"nwp\", password=\"modeli\", dsn=\"192.168.211.62/serv\")\n",
    "    cursor_nwp = conn_nwp.cursor()\n",
    "    \n",
    "    cursor_nwp.execute(\"\"\"\n",
    "    /* SELECT to_char(DATAS, 'YYYY-MM-DD'), ADVANCE_TIME,  TEMP */\n",
    "    SELECT DATAS, ADVANCE_TIME,  TEMP\n",
    "    FROM NWP.GROUND_DATA\n",
    "    WHERE CITY_ID = 936 \n",
    "          AND model_id = 4\n",
    "          AND ADVANCE_TIME between 0 and 48\n",
    "          /*AND datas >= '01-08-2022'*/\n",
    "          /*AND datas between '01-08-2022' and '31-12-2022'*/\n",
    "          AND datas >= :start_date\n",
    "          AND to_char(datas, 'HH24') = '00'\n",
    "    ORDER BY datas, advance_time\n",
    "    \"\"\",\n",
    "    start_date=start_date\n",
    "    )\n",
    "    \n",
    "    date = None\n",
    "    forecast_arr = []\n",
    "    f_row = [] \n",
    "    for c in cursor_nwp:\n",
    "        # Нова дата - создаем новую строку данных\n",
    "        if date != c[0]:\n",
    "            # Если дата пройдена, присоединить строку данных f_row к массиву forecast_arr\n",
    "            if date:\n",
    "                forecast_arr.append(f_row) \n",
    "            # Инициализируем следующую дату и строку данных\n",
    "            date = c[0]       \n",
    "            f_row = [None for _ in range(50)]\n",
    "            f_row[0] = c[0] # Дату c[0] записать в позицию 0\n",
    "        # В позицию списка, равную заблаговременности c[1] записать температуру c[2]\n",
    "        f_row[c[1] + 1] = c[2]\n",
    "    # Присоединить последнюю строку данных f_row к массиву forecast_arr\n",
    "    forecast_arr.append(f_row)        \n",
    "    # print(*forecast_arr, sep='\\n')\n",
    "    conn_nwp.close()\n",
    "    \n",
    "    \n",
    "    conn_gts = cx_Oracle.connect(user=\"cligts\", password=\"belorgmc\", dsn=\"192.168.211.173/clidb\")\n",
    "    cursor_gts = conn_gts.cursor()\n",
    "    \n",
    "    cursor_gts.execute(\n",
    "    \"\"\"\n",
    "    SELECT DATE_OBS, TEMPDB\n",
    "    FROM gts.synop\n",
    "    WHERE \n",
    "      /*date_obs >= '01-08-2022'*/\n",
    "      /* date_obs between '01-08-2022' and '31-12-2022' */\n",
    "      date_obs >= :start_date\n",
    "      AND station_id = '26850'\n",
    "    \"\"\",\n",
    "    start_date=start_date\n",
    "    )\n",
    "    \n",
    "    gts_rows = cursor_gts.fetchall()\n",
    "    # for c in gts_rows:\n",
    "    #     print(c)\n",
    "    \n",
    "    conn_gts.close()\n",
    "    \n",
    "    \n",
    "    # Формирование y - значений фактической температуры, соответствующих каждой дате на 48 часов вперед\n",
    "    # Поиск каждой совпадающей строки по дате и сроку 00\n",
    "    start = 0\n",
    "    fact_arr = []\n",
    "    forecast_X = []\n",
    "    td48 = timedelta(hours=48)\n",
    "    for f in forecast_arr:\n",
    "        date_nwp = f[0]\n",
    "        # Прервать выполнение, если от даты прогноза до текущего момента \n",
    "        # прошло менее или ровно 48 часов, т.к. не будет соответствующих фактических данных\n",
    "        if datetime.today() - date_nwp <= td48:\n",
    "            print(datetime.today() - date_nwp)\n",
    "            print(f'Прервано - для даты {date_nwp} не хватит фактических данных!')\n",
    "            break\n",
    "        # print('NWP Date:', date_nwp)\n",
    "        # print('GTS start:', start)\n",
    "        for i, g in enumerate(gts_rows[start:]):\n",
    "            if g[0] == f[0]: # Строка найдена\n",
    "                # print(f[0])\n",
    "                start += i\n",
    "                forecast_X.append(f)\n",
    "                break\n",
    "        # Инициализировать строку данных для фактической температуры 17 значений\n",
    "        g_row = [None for _ in range(18)]\n",
    "        g_row[0] = g[0]\n",
    "        # print('GTS Date:', g[0])\n",
    "        for j in range(17):\n",
    "            gts_date = gts_rows[start + j][0]\n",
    "            gts_temperature = gts_rows[start + j][1]\n",
    "            td = gts_date - date_nwp\n",
    "            # print('Time delta for g_row:', td, 'Temper;', gts_temperature)\n",
    "            # В пределах 48 часов\n",
    "            if td <= td48:\n",
    "                td_hours = int(td.days * 24 + td.seconds / 3600)\n",
    "                # print('td_hours', td_hours)\n",
    "                cell = td_hours // 3\n",
    "                # print('cell', cell)\n",
    "                g_row[cell + 1] = gts_temperature\n",
    "            else:\n",
    "                # start += i #!!!!\n",
    "                print('BREAK!!!')\n",
    "                break\n",
    "        fact_arr.append(g_row)\n",
    "        # print(g_row)\n",
    "    \n",
    "    # print(*forecast_arr, sep='\\n')\n",
    "    # print('Fact:')\n",
    "    # print(*fact_arr, sep='\\n')\n",
    "    \n",
    "    fact_y = fact_arr\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Проверка на пропущенные значения (None)\n",
    "    damaged_rows = set()\n",
    "    \n",
    "    for i, (x, y) in enumerate(zip(forecast_X, fact_y)):\n",
    "        \n",
    "        if x[0] != y[0]:\n",
    "            damaged_rows.add(i)\n",
    "            print('Damaged dates:', x[0], y[0])\n",
    "        \n",
    "        damage_x = False\n",
    "        for xi in x: \n",
    "            if xi is None:\n",
    "                damage_x = True\n",
    "                break\n",
    "        if damage_x:\n",
    "            damaged_rows.add(i)\n",
    "            print('x', x)\n",
    "        \n",
    "        damage_y = False\n",
    "        for yi in y: \n",
    "            if yi is None:\n",
    "                damage_y = True\n",
    "                break\n",
    "        if damage_y:\n",
    "            damaged_rows.add(i)\n",
    "            print('y', y)\n",
    "    \n",
    "    damaged_rows = sorted(list(damaged_rows), reverse=True)\n",
    "    print(damaged_rows)\n",
    "    \n",
    "    # Удаление строк с пропусками\n",
    "    for i in damaged_rows:\n",
    "        forecast_X.pop(i)\n",
    "        fact_y.pop(i)\n",
    "\n",
    "    return forecast_X, fact_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca20855-ce7b-424a-9187-264abf072fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast, fact = read_temperature_from_db('01-11-2022', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ea003-f147-404c-8a6c-a98a7aaac575",
   "metadata": {},
   "source": [
    "#### Функция записи датасетов в файлы CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d5be45f-996a-4423-8960-01a6fc9f4a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(forecast, fact, forecast_model, z=48, append=True):\n",
    "    mode = 'at' if append else 'wt'\n",
    "    # Запись прогноза\n",
    "    forecast_header = ['date_term'] + [f'z_{x}' for x in range(z + 1)]\n",
    "    with open(f'data/forecast_{forecast_model}.csv', mode, newline='') as f_forecast:\n",
    "        forecast_writer = csv.writer(f_forecast, delimiter=';')\n",
    "        if mode == 'wt':\n",
    "            forecast_writer.writerow(forecast_header)\n",
    "        forecast_writer.writerows(forecast)\n",
    "    # Запись факта\n",
    "    fact_header = ['date'] + [f't_{x % 24}' for x in range(0, z + 1, 3)]\n",
    "    with open(f'data/fact.csv', mode, newline='') as f_fact:\n",
    "        fact_writer = csv.writer(f_fact, delimiter=';')\n",
    "        if mode == 'wt':\n",
    "            fact_writer.writerow(fact_header)\n",
    "        fact_writer.writerows(fact)\n",
    "\n",
    "    # Запись файла с датой последних данных в датасетах\n",
    "    date_time = forecast[-1][0]\n",
    "    write_last_date_file(date_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da095f0e-7724-4015-a72f-82df99670421",
   "metadata": {},
   "source": [
    "#### Функция записи файла с датой последних данных в датасетах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6400dc3-10bd-4b61-bd08-a1889a8901ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_last_date_file(date_time):\n",
    "    with open(f'data/last_date.txt', 'wt') as f:\n",
    "        f.write(str(date_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad8c04-bff4-4857-8e72-6311f680d500",
   "metadata": {},
   "source": [
    "#### Функция чтения файла с датой последних данных в датасетах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf3a5e5-61b6-467f-838e-c74eba62d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_last_date_file():\n",
    "    with open(f'data/last_date.txt', 'rt') as f:\n",
    "        date_time_str = f.readline()\n",
    "        date_time = datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S')\n",
    "    return date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b0ac367-8ba8-4d15-b3a0-c2d1b95f2e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_last_date_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f74fc2-990d-4732-9965-e9a0ea486760",
   "metadata": {},
   "source": [
    "## Функция создания на диске датасетов forecast, fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c897b8a-acf3-47c9-9b05-6e9c31829e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(start_date, forecast_model, z=48):\n",
    "    forecast, fact = read_temperature_from_db(start_date, '')\n",
    "    write_csv(forecast, fact, forecast_model, z=z, append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8289b00f-3d18-4bf8-a785-ae006ec7c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_datasets('01-11-2022', 'gfs', z=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d5b9dc-34a0-4360-967d-259e7c948695",
   "metadata": {},
   "source": [
    "## Функция обновления датасетов новыми данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b39fdf2-59f1-402c-b6d0-1164cc11ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_datasets(forecast_model, z=48):\n",
    "    update_date_time = read_last_date_file() + timedelta(hours=24)\n",
    "    date_str = update_date_time.strftime('%d-%m-%Y')    \n",
    "    forecast, fact = read_temperature_from_db(date_str, '')\n",
    "    if forecast:\n",
    "        write_csv(forecast, fact, forecast_model, z=z, append=True)\n",
    "    else:\n",
    "        print(f'Прервано - для даты {update_date_time} не хватит фактических данных!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1ee8518-5ee9-486f-b01c-86c39865a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_datasets('gfs', z=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ccb54c-34f0-42f7-9f75-8f025e3070d4",
   "metadata": {},
   "source": [
    "#### Функция чтения датасетов из csv файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c041bb9c-7706-49ff-a337-cc050c9b365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_datasets(forecast_model, train_date=None):\n",
    "    if train_date:\n",
    "        train_datetime_obj = datetime.strptime(train_date, '%d-%m-%Y')\n",
    "\n",
    "    forecast = []\n",
    "    with open(f'data/forecast_{forecast_model}.csv', 'rt', newline='') as f_forecast:\n",
    "        forecast_reader = csv.reader(f_forecast, delimiter=';')\n",
    "        forecast_header = next(forecast_reader)\n",
    "        for row in forecast_reader:\n",
    "            r = []\n",
    "            for i, e in enumerate(row):\n",
    "                if i == 0:\n",
    "                    row_date = datetime.strptime(e, '%Y-%m-%d %H:%M:%S')\n",
    "                    if train_date and row_date > train_datetime_obj:\n",
    "                        break  \n",
    "                    r.append(row_date)\n",
    "                else:\n",
    "                    r.append(float(e))\n",
    "            else:\n",
    "                forecast.append(r)\n",
    "                continue\n",
    "            break\n",
    "\n",
    "    fact = []\n",
    "    with open('data/fact.csv', 'rt', newline='') as f_fact:\n",
    "        fact_reader = csv.reader(f_fact, delimiter=';')\n",
    "        fact_header = next(fact_reader)\n",
    "        for row in fact_reader:\n",
    "            r = []\n",
    "            for i, e in enumerate(row):\n",
    "                if i == 0:\n",
    "                    row_date = datetime.strptime(e, '%Y-%m-%d %H:%M:%S')\n",
    "                    if train_date and row_date > train_datetime_obj:\n",
    "                        break  \n",
    "                    r.append(row_date)\n",
    "                else:\n",
    "                    r.append(float(e))\n",
    "            else:\n",
    "                fact.append(r)\n",
    "                continue\n",
    "            break\n",
    "\n",
    "    return forecast, fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7206f7a-c1f8-4065-8dd5-ef8fd20f2a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast, fact = read_datasets('gfs')\n",
    "# print(len(forecast))\n",
    "# print(len(fact))\n",
    "# print(forecast[-1])\n",
    "# print(fact[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5bea3e-c8fa-4b1a-b6b7-1c5ab69e69ef",
   "metadata": {},
   "source": [
    "#### Функция подготовки X и y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e891e27-3877-4bbb-8957-41296349c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy(forecast, fact):\n",
    "    print('get_xy1')\n",
    "    print(len(forecast))\n",
    "    print(len(fact))\n",
    "    X = np.array([xx[1:] for xx in forecast])\n",
    "    y = np.array([yy[1:] for yy in fact])\n",
    "    print(\"get_xy2\")\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4160ef05-7cc5-45e2-863a-0f2c99d7748c",
   "metadata": {},
   "source": [
    "#### Функция интерполяции строк целевых значений y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8983195c-e194-4027-b29b-5b7e893f1fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_y_rows(arr, knots, degree=2):\n",
    "    x = np.linspace(0, arr.shape[1], arr.shape[1])\n",
    "    x_knots = np.linspace(0, arr.shape[1], knots)\n",
    "    arr_new = []\n",
    "    for y in arr:\n",
    "        x_spl = splrep(x, y, k=degree)\n",
    "        y2 = splev(x_knots, x_spl)\n",
    "        arr_new.append(y2)\n",
    "\n",
    "    \n",
    "    return np.array(arr_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcab432-810b-4b43-82ee-963bbd2a97c4",
   "metadata": {},
   "source": [
    "#### Функция аугментации данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecf78dd5-51a2-4afb-b266-42b7123d9c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(x_data, y_data, knots, degree=2):\n",
    "    data_len = len(y_data)\n",
    "    prior_points = np.linspace(0, data_len, data_len)\n",
    "    \n",
    "    xy = []\n",
    "    for z in (x_data, y_data):\n",
    "        z_splitted = np.hsplit(z, z.shape[1])\n",
    "        #print(x_splitted)\n",
    "        z_list = []\n",
    "        for arr in z_splitted:\n",
    "            z_spl = splrep(prior_points, arr, k=degree)\n",
    "            z_points_n = np.linspace(0, data_len, knots)\n",
    "            z_col_augmented = splev(z_points_n, z_spl)\n",
    "            z_list.append(z_col_augmented)\n",
    "        z_augmented = np.array(z_list).T\n",
    "        xy.append(z_augmented)\n",
    "    \n",
    "    x_result, y_result = xy\n",
    "    return x_result, y_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e574ecc2-3e88-44ad-8c8e-410d33e00f72",
   "metadata": {},
   "source": [
    "#### Функция получения трансформеров данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7b7fbd8-0f71-4655-b501-5461620286e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer(transformer, n_samples=10_000):\n",
    "    scaler = (\n",
    "        StandardScaler() if transformer == 'standard' else \\\n",
    "        MinMaxScaler() if transformer == 'minmax' else \\\n",
    "        MaxAbsScaler() if transformer == 'maxabs' else \\\n",
    "        RobustScaler() if transformer == 'robust' else \\\n",
    "        QuantileTransformer(output_distribution='uniform', n_quantiles=min(10_000, n_samples), random_state=0) if transformer == 'uniform' else \\\n",
    "        QuantileTransformer(output_distribution='normal', n_quantiles=min(10_000, n_samples), random_state=0) if transformer == 'normal' else \\\n",
    "        PowerTransformer(method='box-cox', standardize=False) if transformer == 'normal-bc' else \\\n",
    "        PowerTransformer(method='yeo-johnson', standardize=False) if transformer == 'normal-yj' else \\\n",
    "        PowerTransformer(method='box-cox', standardize=True) if transformer == 'normal-bc-st' else \\\n",
    "        PowerTransformer(method='yeo-johnson', standardize=True) if transformer == 'normal-yj-st' else \\\n",
    "        None\n",
    "    )\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e7eca7-8256-49bd-abbe-d1d27782a183",
   "metadata": {},
   "source": [
    "#### Функция вычисления качества моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb2a9988-2dc5-4644-8fb7-9e342d9400b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality(y_true, predicted):\n",
    "    q = {}\n",
    "    q[\"r2_score\"] = r2_score(y_true, predicted) #q_max_error \n",
    "    # q[max_error] = max_error(y_true, predicted) #q_max_error \n",
    "    q[\"mean_absolute_error\"] = mean_absolute_error(y_true, predicted) #q_mean_absolute_error\n",
    "    q[\"mean_squared_error\"] = mean_squared_error(y_true, predicted) #q_mean_squared_error \n",
    "    # q[mean_squared_log_error] = mean_squared_log_error(y_true, predicted) #q_mean_squared_log_error \n",
    "    q[\"median_absolute_error\"] = median_absolute_error(y_true, predicted) #q_median_absolute_error \n",
    "\n",
    "    print(\n",
    "    f'r2_score: {q[\"r2_score\"]}',\n",
    "    # f'max_error: {q[1]}',\n",
    "    f'mean_absolute_error: {q[\"mean_absolute_error\"]}',\n",
    "    f'mean_squared_error: {q[\"mean_squared_error\"]}',\n",
    "    # f'mean_squared_log_error: {q[4]}',\n",
    "    f'median_absolute_error: {q[\"median_absolute_error\"]}',\n",
    "    sep='\\n')\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5de05b1-4af3-432e-a0a3-3a1f8bf6a788",
   "metadata": {},
   "source": [
    "#### Функция записи моделей в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "331e7f4f-33b2-4e5c-ac75-97a3c33413e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model(model_obj, forecast_model, date_str):\n",
    "    with open(f'data/{forecast_model}_{date_str}.bin', 'wb') as f:\n",
    "        model_bin = pickle.dump(model_obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775ef532-4e1a-48c2-a41f-9a05896964a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7b2befc-3734-4fba-acff-a5c623b663d4",
   "metadata": {},
   "source": [
    "## Функция обучения моделей и записи в виде файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b51192ad-3f11-4691-8ae1-1e90b5927e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(forecast_model, train_date=None):\n",
    "\n",
    "    # Чтение данных из датасетов csv\n",
    "    forecast, fact = read_datasets(forecast_model, train_date)\n",
    "    print('train_models')\n",
    "    print(len(forecast))\n",
    "    print(len(fact))\n",
    "    \n",
    "    # Получение первичных данных X, y\n",
    "    X_prior, y_prior = get_xy(forecast, fact)\n",
    "    \n",
    "    # Интерполяция фактических значений\n",
    "    y = interpolate_y_rows(y_prior, 49, degree=2)\n",
    "    y_points_prior = np.linspace(0, y_prior.shape[1], y_prior.shape[1])\n",
    "    y_points = np.linspace(0, y_prior.shape[1], y.shape[1])\n",
    "    \n",
    "    # plt.plot(y_points, y[-1], 'o', y_points_int, y_int[-1])\n",
    "    plt.plot(y_points_prior, y_prior[-13], 'o')\n",
    "    plt.plot(y_points, y[-13])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Аугментация данных\n",
    "    X_aug, y_aug = augment_data(X_prior, y, 4000, degree=1)\n",
    "    print(X_aug.shape, y_aug.shape)\n",
    "    \n",
    "    \n",
    "    # # Разделение на тренировочный и тестовый набор\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "    # print(X_train)\n",
    "    X, y = shuffle(X_aug, y_aug, random_state=42)\n",
    "    \n",
    "    # Условия масштабирования\n",
    "    # scaler_x = 'normal-yj-st'\n",
    "    # scaler_y = None\n",
    "\n",
    "    scaler_x = 'stanard'\n",
    "    scaler_y = 'normal'\n",
    "    \n",
    "    # Подготовка трансформеров\n",
    "    transformer_x = get_transformer(scaler_x, n_samples=y.shape[0]) # !!!\n",
    "    transformer_y = get_transformer(scaler_y, n_samples=y.shape[0]) # !!!\n",
    "    \n",
    "    # Подготовка модели\n",
    "    # model = KNeighborsRegressor(n_neighbors = 7)\n",
    "    # model = RidgeCV()\n",
    "    # model = ExtraTreesRegressor(n_estimators=150, random_state=0)\n",
    "    # model = RandomForestRegressor(n_estimators=150, random_state=0)\n",
    "    # model = GaussianProcessRegressor(kernel=DotProduct() + WhiteKernel(), random_state=0)\n",
    "    # model = MLPRegressor(hidden_layer_sizes=(49, 21, 8, 21, 49), random_state=0, max_iter=1000, alpha=0.0001, activation='relu') # +0.42247\n",
    "    # model = MLPRegressor(hidden_layer_sizes=(100, 42, 8, 42, 100), random_state=0, max_iter=1000, alpha=0.0001, activation='relu')\n",
    "    model = MLPRegressor(hidden_layer_sizes=(100, 42, 8, 42, 100), random_state=0, max_iter=1000, alpha=0.00005, activation='relu', early_stopping=True) # +0.9473 4000\n",
    "    \n",
    "    # Трансформация целевых значений y\n",
    "    if scaler_y: \n",
    "        regressor = TransformedTargetRegressor(regressor=model, transformer=transformer_y)\n",
    "    else:\n",
    "        regressor = model\n",
    "    \n",
    "    # Декомпозиция\n",
    "    # decomposition = PCA(n_components='mle', random_state=0, whiten = True)\n",
    "    decomposition = PCA(n_components=None, random_state=0, whiten = False, svd_solver='randomized') #!! + NN\n",
    "    # decomposition = IncrementalPCA(n_components=None, whiten = True)\n",
    "    # decomposition = KernelPCA(\n",
    "    #     n_components=None, kernel=\"rbf\", gamma=10, fit_inverse_transform=True, alpha=0.1\n",
    "    # )\n",
    "    \n",
    "    # Трансформация признаков X\n",
    "    regr = make_pipeline(transformer_x, decomposition, regressor) if transformer_x else regressor\n",
    "    # regr = make_pipeline(transformer_x, regressor) if transformer_x else regressor\n",
    "    \n",
    "    # Обучение моделей\n",
    "    regr.fit(X, y)\n",
    "    \n",
    "    # Запись модели в файл\n",
    "    date_str = datetime.strftime(forecast[-1][0], '%Y-%m-%d-%H')\n",
    "    write_model(regr, forecast_model, date_str)\n",
    "    \n",
    "    # Оценка качества модели\n",
    "    predicted = regr.predict(X)\n",
    "    r2 = regr.score(X, y)\n",
    "    print(r2)\n",
    "    _ = quality(y, predicted)\n",
    "\n",
    "\n",
    "    # regr_p= pickle.loads(pickle.dumps(regr))\n",
    "    # # Оценка качества модели\n",
    "    # predicted = regr_p.predict(X)\n",
    "    # r2 = regr_p.score(X, y)\n",
    "    # print(r2)\n",
    "    # _ = quality(y, predicted)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f77bceb2-be0c-4a91-ac01-3f8d32670b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_models('gfs', '21-01-2024')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f2d9df-d5a9-419f-bdb0-59e4390ba4cd",
   "metadata": {},
   "source": [
    "#### Функция чтения очередной строки данных для прогноза из БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdd28258-427a-4cbf-9d38-7fb49d6cf3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_last_X(forecast_model, forecast_date):\n",
    "    \n",
    "    # Коды моделей\n",
    "    models = {\n",
    "        'gfs': 4,\n",
    "        'icon': 5,\n",
    "        'orv': 100,\n",
    "    }\n",
    "\n",
    "    # Запрос\n",
    "    conn_nwp = oracledb.connect(user=\"nwp\", password=\"modeli\", dsn=\"192.168.211.62/serv\")\n",
    "    cursor_nwp = conn_nwp.cursor()\n",
    "    \n",
    "    cursor_nwp.execute(\"\"\"\n",
    "    SELECT TEMP\n",
    "    FROM NWP.GROUND_DATA\n",
    "    WHERE CITY_ID = 936 \n",
    "          AND model_id = :model_id\n",
    "          AND ADVANCE_TIME between 0 and 48\n",
    "          AND datas = :start_date\n",
    "          AND to_char(datas, 'HH24') = '00'\n",
    "    ORDER BY datas, advance_time\n",
    "    \"\"\",\n",
    "    start_date=forecast_date,\n",
    "    model_id = models[forecast_model]          \n",
    "    )\n",
    "\n",
    "    db_data = cursor_nwp.fetchall()\n",
    "    data = np.array([x[0] for x in db_data]).reshape((1, -1))\n",
    "    print(data)\n",
    "    # print(bool(data))\n",
    "    print(data.size)\n",
    "\n",
    "    conn_nwp.close()\n",
    "\n",
    "    return data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25ce9b95-a264-4710-a5b4-8d1b4a427c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_last_X('orv', '09-01-2024')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93eaae-1bfc-4448-a4c2-22251c88a5ad",
   "metadata": {},
   "source": [
    "#### Функция получения модели из файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "511760bb-b64b-45e3-8f43-74b623ee1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model(forecast_model, model_date):\n",
    "\n",
    "    # Получение списка моделей\n",
    "    files = filter(lambda f: '.bin' in f, os.listdir('./data'))\n",
    "    files = sorted(files, key=lambda f: datetime.strptime(f.split('_')[1].split('.')[0], '%Y-%m-%d-%H'), reverse=True)\n",
    "    # print(files)\n",
    "\n",
    "    # Вычисление строки даты в имени файла\n",
    "    arg_datetime = datetime.strptime(model_date, '%d-%m-%Y')\n",
    "    file_date = datetime.strftime(arg_datetime, '%Y-%m-%d')\n",
    "\n",
    "    # Поиск имени файла, содержащего дату\n",
    "    model_file = tuple(filter(lambda f: file_date in f, files))[0]\n",
    "    print(model_file)\n",
    "    \n",
    "    # last_model_file = files[0]\n",
    "    with open(f'data/{model_file}', 'rb') as m:\n",
    "        model = pickle.load(m)\n",
    "\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55ff5f80-cca4-4585-95b2-98141240745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_model('gfs', '05-01-2024')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376bca3-2ae9-442e-ae99-fe607c382147",
   "metadata": {},
   "source": [
    "#### Функция вычисления прогноза на текущей строке данных X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2466c167-2cae-4f1d-969e-4a062183818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_forecast(forecast_model, forecast_date):\n",
    "    # Чтение последних данных X \n",
    "    X = read_last_X(forecast_model, forecast_date)\n",
    "\n",
    "    # Получение модели из файла\n",
    "    arg_datetime = datetime.strptime(forecast_date, '%d-%m-%Y')\n",
    "    model_datetime = arg_datetime - timedelta(hours=48)\n",
    "    model_date = datetime.strftime(model_datetime, '%d-%m-%Y')\n",
    "    model = read_model(forecast_model, model_date)\n",
    "\n",
    "    print('Model date', model_date)\n",
    "        \n",
    "    # Прогноз по модели\n",
    "    predicted = model.predict(X)\n",
    "    # print(predicted)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfa545a9-be4f-4abb-b792-8beca556c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_forecast('gfs', '07-01-2024')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ba65a-883b-4f20-86df-d1f0ddcb7390",
   "metadata": {},
   "source": [
    "#### Функция прогнозирования на дату и записи прогноза в БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f935b731-1e96-40a0-9825-c6ae7bff81f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_forecast_to_db(forecast_model, forecast_date):\n",
    "    \n",
    "    # Коды моделей\n",
    "    models = {\n",
    "        'gfs': 4,\n",
    "        'icon': 5,\n",
    "    }\n",
    "\n",
    "    # Вычислить прогноз на дату\n",
    "    temp_forecast = np.reshape(make_forecast(forecast_model, forecast_date),(-1, ))\n",
    "    \n",
    "    print('temp_forecast:')\n",
    "    print(temp_forecast)\n",
    "\n",
    "    # Сформировать список кортежей (строк) для записи в БД\n",
    "    # (DATAS, MODEL_ID, ADVANCE_TIME, CITY_ID, TEMP)\n",
    "    datas = datetime.strptime(forecast_date, '%d-%m-%Y')\n",
    "    model_id = 100\n",
    "    city_id = 936\n",
    "    rows_list = []\n",
    "    for advance_time, temp in enumerate(temp_forecast):\n",
    "        row = (temp, datas, model_id, advance_time, city_id)\n",
    "        rows_list.append(row)\n",
    "        \n",
    "    print(rows_list) \n",
    "    \n",
    "    # Определение вида операции с БД - INSERT или UPDATE\n",
    "    data_on_date = read_last_X('orv', forecast_date)\n",
    "    if data_on_date.size == 0:\n",
    "        # Insert statement\n",
    "        sql = \"\"\"\n",
    "        INSERT INTO nwp.GROUND_DATA (TEMP, DATAS, MODEL_ID, ADVANCE_TIME, CITY_ID)\n",
    "        VALUES (:1, :2, :3, :4, :5)\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # Update statement\n",
    "        sql = \"\"\"\n",
    "        UPDATE nwp.GROUND_DATA\n",
    "        SET \n",
    "            TEMP = :1\n",
    "        WHERE \n",
    "            DATAS = :2 and\n",
    "            MODEL_ID = :3 and\n",
    "            ADVANCE_TIME = :4 and\n",
    "            CITY_ID = :5\n",
    "        \"\"\"\n",
    "    \n",
    "    # Запрос\n",
    "    conn_nwp = oracledb.connect(user=\"nwp\", password=\"modeli\", dsn=\"192.168.211.62/serv\")\n",
    "    cursor_nwp = conn_nwp.cursor()\n",
    "    cursor_nwp.executemany(sql, rows_list)\n",
    "    conn_nwp.commit()\n",
    "\n",
    "    conn_nwp.close()\n",
    "\n",
    "    # return data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39e58b45-97a9-4289-9f3a-0023c2e77271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_forecast_to_db('gfs', '23-01-2024')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cea0ae26-e9cb-47d9-a1c5-528340b5005a",
   "metadata": {},
   "source": [
    "y_points = np.linspace(0, y.shape[1], y.shape[1])\n",
    "# y_points_int = np.linspace(0, y.shape[1], y_int.shape[1])\n",
    "x_points = np.linspace(0, y.shape[1], X.shape[1])\n",
    "\n",
    "plt.plot(y_points, y_test[-25], label='Факт')\n",
    "plt.plot(y_points, predicted[-25], label='Прогноз')\n",
    "plt.plot(x_points, X_test[-25], label='GFS')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce4698d3-4d89-43eb-8752-6986a1cf4ffe",
   "metadata": {},
   "source": [
    "# test_data = np.vstack((y_test[20], predicted[20])).T\n",
    "test_data = np.vstack((y_test[-25], predicted[-25], X_test[-25])).T\n",
    "# test_data = np.vstack((y_test[-1], y_test[-2], X_test[-1][::3])).T\n",
    "sns.lineplot(data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2770755-4bd4-4252-a19f-41011b44fbb2",
   "metadata": {},
   "source": [
    "#### Функция разделения на train и test с чередованием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31c996a1-30e5-4b6a-bcc9-91e7d943ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_alternation(x, y, test_size, step=2):\n",
    "    print('train_test_split_alternation')\n",
    "    print('x', x.shape)\n",
    "    print('y', y.shape)\n",
    "    x_test = np.empty((0, 49))\n",
    "    y_test = np.empty((0, 49))\n",
    "    print(x_test, x_test)\n",
    "    to_del = []\n",
    "    for i in range(-1, -test_size * step, -step):\n",
    "        try:\n",
    "            # xv = x_train.pop(i)\n",
    "            # yv = y_train.pop(i)\n",
    "            xi = x[i]\n",
    "            xi = xi[np.newaxis, ...]\n",
    "            yi = y[i]\n",
    "            yi = yi[np.newaxis, ...]\n",
    "            x_test = np.concatenate((x_test, xi), axis=0)\n",
    "            y_test = np.concatenate((y_test, yi), axis=0)\n",
    "            to_del.append(i)\n",
    "        except IndexError:\n",
    "            break\n",
    "\n",
    "    x_test = np.flipud(x_test)\n",
    "    y_test = np.flipud(y_test)\n",
    "    x_train = np.delete(x, to_del, 0)\n",
    "    y_train = np.delete(y, to_del, 0)\n",
    "    print('train_test_split_alternation')\n",
    "    print('x_test', x_test.shape)\n",
    "    print('y_test', y_test.shape)\n",
    "    print('x_train', x_train.shape)\n",
    "    print('y_train', y_train.shape)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2e62058e-5fcd-4677-901b-ec787ecdc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lines(X_test, y_test, predicted, n=-1):\n",
    "    # Графики линий\n",
    "    hours = np.arange(49)\n",
    "    plt.plot(hours, y_test[n], label='Факт')\n",
    "    plt.plot(hours, predicted[n], label='Прогноз')\n",
    "    plt.plot(hours, X_test[n, 5:], label='GFS')\n",
    "    # plt.plot(hours, X_test[n], label='GFS')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43748058-2c8b-4d6f-b76e-a45afcef135e",
   "metadata": {},
   "source": [
    "## Функция оценки моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "8c27d3cc-0ecc-4c78-a849-803cdb4fe42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models():\n",
    "\n",
    "    # Чтение данных из датасетов csv\n",
    "    forecast, fact = read_datasets('gfs')\n",
    "    \n",
    "    # Получение первичных данных X, y\n",
    "    X_prior, y_prior = get_xy(forecast, fact)\n",
    "    \n",
    "    # Интерполяция фактических значений\n",
    "    y = interpolate_y_rows(y_prior, 49, degree=2)\n",
    "    y_points_prior = np.linspace(0, y_prior.shape[1], y_prior.shape[1])\n",
    "    y_points = np.linspace(0, y_prior.shape[1], y.shape[1])\n",
    "    \n",
    "    # # plt.plot(y_points, y[-1], 'o', y_points_int, y_int[-1])\n",
    "    # plt.plot(y_points_prior, y_prior[-13], 'o')\n",
    "    # plt.plot(y_points, y[-13])\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # Разделение на тренировочный и тестовый набор Временно!\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X_prior, y, test_size=35, shuffle=False, random_state=42) # Было 15\n",
    "    X_train, X_test, y_train, y_test = train_test_split_alternation(X_prior, y, test_size=94, step=4)\n",
    "    \n",
    "\n",
    "    # Ассимиляция данных 0.92292\n",
    "    X_train = np.hstack((y_train[:, 0:5], X_train))\n",
    "    X_test = np.hstack((y_test[:, 0:5], X_test))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Аугментация данных Временно!\n",
    "    X_train, y_train = augment_data(X_train, y_train, 4000, degree=1)\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    \n",
    "    \n",
    "    # # Разделение на тренировочный и тестовый набор\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.005, shuffle=False, random_state=42)\n",
    "    # Перемешивание\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "    \n",
    "    print('X_test.shape', X_test.shape)\n",
    "    \n",
    "    # Условия масштабирования 0.864795\n",
    "    # scaler_x = 'standard' #'normal' #'normal-yj-st' #None #'normal-yj-st'\n",
    "    # scaler_y = 'normal' #'normal' #'normal-yj-st' #None #'standard' # None\n",
    "\n",
    "    # 0.906089\n",
    "    scaler_x = 'normal-yj-st'\n",
    "    scaler_y = None\n",
    "\n",
    "    # # 0.85912\n",
    "    # scaler_x = 'normal-yj-st'\n",
    "    # scaler_y = 'normal-yj-st'\n",
    "\n",
    "    # # 0.8574014\n",
    "    # scaler_x = 'normal-yj-st'\n",
    "    # scaler_y = 'normal'\n",
    "    \n",
    "    # # 0.86674\n",
    "    # scaler_x = 'normal-yj-st'\n",
    "    # scaler_y = 'normal-yj'\n",
    "\n",
    "    # # 0.8508759\n",
    "    # scaler_x = 'standard'\n",
    "    # scaler_y = 'standard'\n",
    "\n",
    "    # 0.876117\n",
    "    # scaler_x = 'normal-yj-st'\n",
    "    # scaler_y = 'standard'\n",
    "    \n",
    "\n",
    "    # 'normal''normal' 0.7364\n",
    "    # None None 0.58707\n",
    "    # 'normal-yj-st' 'normal-yj-st' 0.778169\n",
    "    # 'normal' 'normal'0.73643\n",
    "    # 'standard' 'standard' 0.788689\n",
    "    # 'standard' 'normal'0.753527\n",
    "    # 'normal' 'standard' 0.130696\n",
    "    # 'minmax' 'minmax' 0.56305\n",
    "    # 'maxabs' 'maxabs' -35.48\n",
    "    # test_size=15\n",
    "    # 'normal-yj-st' none 0.71757\n",
    "    # 'standard' None 0.72584\n",
    "    # 'standard' 'standard' 0.7692\n",
    "    # 'normal-yj-st' 'normal-yj-st' 0.74478\n",
    "    # 'normal-yj-st' 'standard' 0.76224\n",
    "    # 'normal' 'standard' 0.0092\n",
    "    # 'standard' 'normal' 0.771224\n",
    "    \n",
    "    \n",
    "    # Подготовка трансформеров\n",
    "    transformer_x = get_transformer(scaler_x, n_samples=y_train.shape[0]) # !!!\n",
    "    transformer_y = get_transformer(scaler_y, n_samples=y_train.shape[0]) # !!!\n",
    "    \n",
    "    # Подготовка модели\n",
    "    # model = KNeighborsRegressor(n_neighbors = 5)#-3.65  0.75466 4000\n",
    "    # model = RidgeCV() #-2.20 +0.78115 4000\n",
    "    model = ExtraTreesRegressor(n_estimators=150, random_state=0) #-6.05 +0.8307 3000\n",
    "    # model = RandomForestRegressor(n_estimators=150, random_state=0) #-4.55\n",
    "    # model = GaussianProcessRegressor(kernel=DotProduct() + WhiteKernel(), random_state=0) #-2.25\n",
    "\n",
    "    \n",
    "   \n",
    "    # model = MLPRegressor(hidden_layer_sizes=(49, 21, 8, 21, 49), random_state=0, max_iter=1000, alpha=0.0001, activation='relu') # +0.42247 !!! +0.84355 4000    \n",
    "    # model = MLPRegressor(hidden_layer_sizes=(100, 42, 8, 42, 100), random_state=0, max_iter=5000, alpha=0.00005, activation='relu', early_stopping=True) # +0.9473 4000 !!!! 0.9427 12000\n",
    "\n",
    "    \n",
    "    # Трансформация целевых значений y\n",
    "    if scaler_y: \n",
    "        regressor = TransformedTargetRegressor(regressor=model, transformer=transformer_y)\n",
    "    else:\n",
    "        regressor = model\n",
    "    \n",
    "    # Декомпозиция\n",
    "    # decomposition = PCA(n_components='mle', random_state=0, whiten = True)\n",
    "    # decomposition = PCA(n_components=None, random_state=0, whiten = True, svd_solver='randomized') #!!\n",
    "    decomposition = PCA(n_components=None, random_state=0, whiten = False, svd_solver='randomized') #!! + NN\n",
    "    # decomposition = IncrementalPCA(n_components=None, whiten = True)\n",
    "    # decomposition = KernelPCA(\n",
    "    #     n_components=None, kernel=\"rbf\", gamma=10, fit_inverse_transform=True, alpha=0.1\n",
    "    # )\n",
    "    \n",
    "    # Трансформация признаков X\n",
    "    # regr = make_pipeline(transformer_x, decomposition, regressor) if transformer_x else regressor\n",
    "    # regr = make_pipeline(transformer_x, regressor) if transformer_x else regressor\n",
    "\n",
    "    regr = ExtraTreesRegressor(bootstrap=False, max_features=0.2, min_samples_leaf=1, min_samples_split=7, n_estimators=110, random_state=0) #TPOT !!!! 0.9529\n",
    "    \n",
    "    \n",
    "    # Обучение моделей\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    # Запись модели в файл\n",
    "    \n",
    "    # Оценка качества модели\n",
    "    predicted = regr.predict(X_test)\n",
    "    # r2 = regr.score(X_test, y_test)\n",
    "    # print(r2)\n",
    "    _ = quality(y_test, predicted)\n",
    "\n",
    "    # Оценка качества исходного прогноза\n",
    "    _ = quality(X_train[:, 5:], y_train)\n",
    "    # _ = quality(X_train[:, -1], y_train)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return X_test, y_test, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "af8be15b-b063-4829-a3b0-ece6d0a34d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_xy1\n",
      "373\n",
      "373\n",
      "get_xy2\n",
      "(373, 49)\n",
      "(373, 17)\n",
      "train_test_split_alternation\n",
      "x (373, 49)\n",
      "y (373, 49)\n",
      "[] []\n",
      "train_test_split_alternation\n",
      "x_test (94, 49)\n",
      "y_test (94, 49)\n",
      "x_train (279, 49)\n",
      "y_train (279, 49)\n",
      "(4000, 54) (4000, 49)\n",
      "X_test.shape (94, 54)\n",
      "r2_score: 0.9529838866802399\n",
      "mean_absolute_error: 1.417089268614103\n",
      "mean_squared_error: 3.5084243519670677\n",
      "median_absolute_error: 1.1288773284850557\n",
      "r2_score: 0.9601428002840064\n",
      "mean_absolute_error: 1.378668431416358\n",
      "mean_squared_error: 3.4399017582013\n",
      "median_absolute_error: 1.0704255354764212\n",
      "X_test (94, 54)\n",
      "y_test (94, 49)\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test, predicted = evaluate_models()\n",
    "print('X_test', X_test.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "430ef6e9-6a1c-4de5-945a-cf646006ce33",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -134 is out of bounds for axis 0 with size 94",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[233], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m134\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[67], line 4\u001b[0m, in \u001b[0;36mplot_lines\u001b[1;34m(X_test, y_test, predicted, n)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_lines\u001b[39m(X_test, y_test, predicted, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Графики линий\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     hours \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m49\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(hours, \u001b[43my_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mФакт\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(hours, predicted[n], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mПрогноз\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(hours, X_test[n, \u001b[38;5;241m5\u001b[39m:], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGFS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index -134 is out of bounds for axis 0 with size 94"
     ]
    }
   ],
   "source": [
    "plot_lines(X_test, y_test, predicted, n=-134)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be4a15d0-ada8-4e08-929b-953c8fcb98ac",
   "metadata": {},
   "source": [
    "r2_score: 0.9060891989266867\n",
    "mean_absolute_error: 1.4635624853231644\n",
    "mean_squared_error: 3.9784362032601295\n",
    "median_absolute_error: 1.1546912847756665\n",
    "r2_score: 0.9581153272796709\n",
    "mean_absolute_error: 1.3765114852753584\n",
    "mean_squared_error: 3.4687148816173554\n",
    "median_absolute_error: 1.0490620728818718\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7aaf6096-a614-4193-8610-eb39fc77e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d77b3-0457-4891-9500-bb0a4b912c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Обучение по последней точке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b173f7b8-41e9-46c4-9c0e-ace24e2210b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models():\n",
    "\n",
    "    # Чтение данных из датасетов csv\n",
    "    forecast, fact = read_datasets('gfs')\n",
    "    \n",
    "    # Получение первичных данных X, y\n",
    "    X_prior, y_prior = get_xy(forecast, fact)\n",
    "    \n",
    "    # Интерполяция фактических значений\n",
    "    y = interpolate_y_rows(y_prior, 49, degree=2)\n",
    "    y_points_prior = np.linspace(0, y_prior.shape[1], y_prior.shape[1])\n",
    "    y_points = np.linspace(0, y_prior.shape[1], y.shape[1])\n",
    "    \n",
    "    # # plt.plot(y_points, y[-1], 'o', y_points_int, y_int[-1])\n",
    "    # plt.plot(y_points_prior, y_prior[-13], 'o')\n",
    "    # plt.plot(y_points, y[-13])\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # Разделение на тренировочный и тестовый набор Временно!\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X_prior, y, test_size=35, shuffle=False, random_state=42) # Было 15\n",
    "    X_train, X_test, y_train, y_test = train_test_split_alternation(X_prior, y, test_size=135, step=5)\n",
    "    \n",
    "\n",
    "    # Ассимиляция данных 0.92292\n",
    "    X_train = np.hstack((y_train[:, 0:5], X_train))\n",
    "    X_test = np.hstack((y_test[:, 0:5], X_test))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Аугментация данных Временно!\n",
    "    X_train, y_train = augment_data(X_train, y_train, 4000, degree=1) #4000 745\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    \n",
    "    \n",
    "    # # Разделение на тренировочный и тестовый набор\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.005, shuffle=False, random_state=42)\n",
    "    # Перемешивание\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "    \n",
    "    y_train = y_train[:, -1]\n",
    "    y_test = y_test[:, -1]\n",
    "    \n",
    "    print('X_test.shape', X_test.shape)\n",
    "\n",
    "    from tpot import TPOTRegressor\n",
    "    \n",
    "    # # TPOT\n",
    "    # tpot = TPOTRegressor(generations=6, population_size=150, verbosity=3, random_state=42, n_jobs=4, memory='auto', scoring='r2')\n",
    "    # tpot.fit(X_train, y_train)\n",
    "    # print(tpot.score(X_test, y_test))\n",
    "    # tpot.export('tpot_gfs_coef_pipeline_49.py')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Условия масштабирования 0.864795\n",
    "    # scaler_x = 'standard' #'normal' #'normal-yj-st' #None #'normal-yj-st'\n",
    "    # scaler_y = 'normal' #'normal' #'normal-yj-st' #None #'standard' # None\n",
    "\n",
    "    # 0.906089\n",
    "    scaler_x = 'normal-yj-st'\n",
    "    scaler_y = None\n",
    "\n",
    "    # # 0.85912\n",
    "    # scaler_x = 'normal-yj-st'\n",
    "    # scaler_y = 'normal-yj-st'\n",
    "\n",
    "    # # 0.8574014\n",
    "    # scaler_x = 'normal-yj-st'\n",
    "    # scaler_y = 'normal'\n",
    "    \n",
    "    # # 0.86674\n",
    "    # scaler_x = 'normal-yj-st'\n",
    "    # scaler_y = 'normal-yj'\n",
    "\n",
    "    # # 0.8508759\n",
    "    # scaler_x = 'standard'\n",
    "    # scaler_y = 'standard'\n",
    "\n",
    "    # 0.876117\n",
    "    # scaler_x = 'normal-yj-st'\n",
    "    # scaler_y = 'standard'\n",
    "    \n",
    "\n",
    "    # 'normal''normal' 0.7364\n",
    "    # None None 0.58707\n",
    "    # 'normal-yj-st' 'normal-yj-st' 0.778169\n",
    "    # 'normal' 'normal'0.73643\n",
    "    # 'standard' 'standard' 0.788689\n",
    "    # 'standard' 'normal'0.753527\n",
    "    # 'normal' 'standard' 0.130696\n",
    "    # 'minmax' 'minmax' 0.56305\n",
    "    # 'maxabs' 'maxabs' -35.48\n",
    "    # test_size=15\n",
    "    # 'normal-yj-st' none 0.71757\n",
    "    # 'standard' None 0.72584\n",
    "    # 'standard' 'standard' 0.7692\n",
    "    # 'normal-yj-st' 'normal-yj-st' 0.74478\n",
    "    # 'normal-yj-st' 'standard' 0.76224\n",
    "    # 'normal' 'standard' 0.0092\n",
    "    # 'standard' 'normal' 0.771224\n",
    "    \n",
    "    \n",
    "    # Подготовка трансформеров\n",
    "    transformer_x = get_transformer(scaler_x, n_samples=y_train.shape[0]) # !!!\n",
    "    transformer_y = get_transformer(scaler_y, n_samples=y_train.shape[0]) # !!!\n",
    "    \n",
    "    # Подготовка модели\n",
    "    # model = KNeighborsRegressor(n_neighbors = 5)#-3.65  0.75466 4000\n",
    "    # model = RidgeCV() #-2.20 +0.78115 4000\n",
    "    # model = ExtraTreesRegressor(n_estimators=150, random_state=0) #-6.05 +0.8307 3000\n",
    "    model = ExtraTreesRegressor(bootstrap=False, max_features=0.2, min_samples_leaf=1, min_samples_split=7, n_estimators=100) #TPOT\n",
    "    # model = RandomForestRegressor(n_estimators=150, random_state=0) #-4.55\n",
    "    # model = GaussianProcessRegressor(kernel=DotProduct() + WhiteKernel(), random_state=0) #-2.25\n",
    "\n",
    "    \n",
    "   \n",
    "    # model = MLPRegressor(hidden_layer_sizes=(49, 21, 8, 21, 49), random_state=0, max_iter=1000, alpha=0.0001, activation='relu') # +0.42247 !!! +0.84355 4000    \n",
    "    # model = MLPRegressor(hidden_layer_sizes=(100, 42, 8, 42, 100), random_state=0, max_iter=5000, alpha=0.00005, activation='relu', early_stopping=True) # +0.9473 4000 !!!! 0.9427 12000\n",
    "\n",
    "    \n",
    "    # Трансформация целевых значений y\n",
    "    if scaler_y: \n",
    "        regressor = TransformedTargetRegressor(regressor=model, transformer=transformer_y)\n",
    "    else:\n",
    "        regressor = model\n",
    "    \n",
    "    # Декомпозиция\n",
    "    # decomposition = PCA(n_components='mle', random_state=0, whiten = True)\n",
    "    # decomposition = PCA(n_components=None, random_state=0, whiten = True, svd_solver='randomized') #!!\n",
    "    decomposition = PCA(n_components=None, random_state=0, whiten = False, svd_solver='randomized') #!! + NN\n",
    "    # decomposition = IncrementalPCA(n_components=None, whiten = True)\n",
    "    # decomposition = KernelPCA(\n",
    "    #     n_components=None, kernel=\"rbf\", gamma=10, fit_inverse_transform=True, alpha=0.1\n",
    "    # )\n",
    "    \n",
    "    # Трансформация признаков X\n",
    "    regr = make_pipeline(transformer_x, decomposition, regressor) if transformer_x else regressor\n",
    "    # regr = make_pipeline(transformer_x, regressor) if transformer_x else regressor\n",
    "\n",
    "    regr = ExtraTreesRegressor(bootstrap=False, max_features=0.2, min_samples_leaf=1, min_samples_split=7, n_estimators=100) #TPOT\n",
    "\n",
    "\n",
    "    # # Average CV score on the training set was: 0.9817920529729014\n",
    "    # regr = make_pipeline(\n",
    "    # StackingEstimator(estimator=AdaBoostRegressor(learning_rate=0.1, loss=\"exponential\", n_estimators=100)),\n",
    "    # PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),\n",
    "    # RidgeCV()\n",
    "    # )\n",
    "    # # Fix random state for all the steps in exported pipeline\n",
    "    # set_param_recursive(regr.steps, 'random_state', 42)\n",
    "\n",
    "\n",
    "    \n",
    "    # Обучение моделей\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    # Запись модели в файл\n",
    "    \n",
    "    # Оценка качества модели\n",
    "    predicted = regr.predict(X_test)\n",
    "    # r2 = regr.score(X_test, y_test)\n",
    "    # print(r2)\n",
    "    _ = quality(y_test, predicted)\n",
    "\n",
    "    # Оценка качества исходного прогноза\n",
    "    _ = quality(X_train[:, -1], y_train)\n",
    "    # _ = quality(X_train, y_train)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return X_test, y_test, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "dc4942ce-269a-4a8c-b1c0-b5a41b0937d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_xy1\n",
      "373\n",
      "373\n",
      "get_xy2\n",
      "(373, 49)\n",
      "(373, 17)\n",
      "train_test_split_alternation\n",
      "x (373, 49)\n",
      "y (373, 49)\n",
      "[] []\n",
      "train_test_split_alternation\n",
      "x_test (75, 49)\n",
      "y_test (75, 49)\n",
      "x_train (298, 49)\n",
      "y_train (298, 49)\n",
      "(4000, 54) (4000, 49)\n",
      "X_test.shape (75, 54)\n",
      "r2_score: 0.941081553969236\n",
      "mean_absolute_error: 1.603272969242306\n",
      "mean_squared_error: 4.067437906935062\n",
      "median_absolute_error: 1.2530063640910463\n",
      "r2_score: 0.93173819591569\n",
      "mean_absolute_error: 1.700611564516128\n",
      "mean_squared_error: 4.881894002036194\n",
      "median_absolute_error: 1.3287160540135137\n",
      "X_test (75, 54)\n",
      "y_test (75,)\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test, predicted = evaluate_models()\n",
    "print('X_test', X_test.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "217a2e18-a01c-437d-9899-ec19240b41f8",
   "metadata": {},
   "source": [
    "Generation 6 - Current Pareto front scores:\n",
    "                                                                                                                       \n",
    "-1\t0.9692453057599622\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
    "                                                                                                                       \n",
    "-2\t0.9791111011541022\tRidgeCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
    "                                                                                                                       \n",
    "-3\t0.9817920529729014\tRidgeCV(PolynomialFeatures(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
    "-0.13234020674024838                                                                                                   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
